{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Limits & Continuity**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Limit**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "The **limit** of $f(x)$ as $x$ approaches $a$ is:\n",
        "\n",
        "$$\n",
        "\\lim_{x \\to a} f(x) = L\n",
        "$$\n",
        "\n",
        "if $f(x)$ gets arbitrarily close to $L$ when $x$ gets close to $a$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Limits**\n",
        "\n",
        "* **Finite:** $\\lim_{x \\to 2} (3x+1) = 7$\n",
        "* **Infinite:** $\\lim_{x \\to \\infty} \\frac{1}{x} = 0$\n",
        "* **One-sided:**\n",
        "\n",
        "  * Left-hand limit: $\\lim_{x \\to a^-} f(x)$\n",
        "  * Right-hand limit: $\\lim_{x \\to a^+} f(x)$\n",
        "\n",
        "---\n",
        "\n",
        "### **Basic Limit Rules**\n",
        "\n",
        "If $\\lim f(x) = A$ and $\\lim g(x) = B$:\n",
        "\n",
        "* Sum: $\\lim (f+g) = A+B$\n",
        "* Product: $\\lim (f \\cdot g) = A \\cdot B$\n",
        "* Quotient: $\\lim \\frac{f}{g} = \\frac{A}{B}$, if $B \\neq 0$\n",
        "\n",
        "---\n",
        "\n",
        "### **Special Limits**\n",
        "\n",
        "* $\\lim_{x \\to 0} \\frac{\\sin x}{x} = 1$\n",
        "* $\\lim_{x \\to \\infty} \\left(1 + \\frac{1}{x}\\right)^x = e$\n",
        "\n",
        "‚úîÔ∏è These appear in derivatives and ML optimization formulas.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Continuity**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "A function $f(x)$ is **continuous at $x=a$** if:\n",
        "\n",
        "1. $f(a)$ is defined.\n",
        "2. $\\lim_{x \\to a} f(x)$ exists.\n",
        "3. $\\lim_{x \\to a} f(x) = f(a)$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Discontinuities**\n",
        "\n",
        "* **Removable:** ‚ÄúHole‚Äù in the graph (e.g., $\\frac{x^2-1}{x-1}$ at $x=1$).\n",
        "* **Jump:** Different left and right limits.\n",
        "* **Infinite:** Goes to $\\pm \\infty$ near a point (vertical asymptote).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "1. $f(x) = x^2$ ‚Üí continuous everywhere.\n",
        "2. $f(x) = \\frac{1}{x}$ ‚Üí discontinuous at $x=0$.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° ML Intuition\n",
        "\n",
        "* **Gradients require limits** (derivative is defined using limits).\n",
        "* **Continuity ensures smooth optimization** ‚Üí gradient descent works well.\n",
        "* **Discontinuous functions** ‚Üí harder to optimize (non-differentiable points).\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Summary\n",
        "\n",
        "| Concept       | Meaning                               | ML Connection                  |\n",
        "| ------------- | ------------------------------------- | ------------------------------ |\n",
        "| Limit         | What function approaches near a point | Foundation of derivatives      |\n",
        "| Continuity    | No jumps, breaks, holes               | Needed for smooth optimization |\n",
        "| Discontinuity | Function undefined / jumps            | Non-smooth loss functions      |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **2. Differentiation Rules**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Definition of Derivative**\n",
        "\n",
        "$$\n",
        "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
        "$$\n",
        "\n",
        "üëâ Measures the **instantaneous rate of change** (slope).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Basic Rules**\n",
        "\n",
        "* **Constant Rule:**\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}[c] = 0\n",
        "$$\n",
        "\n",
        "* **Power Rule:**\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}[x^n] = nx^{n-1}\n",
        "$$\n",
        "\n",
        "* **Constant Multiple:**\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}[c f(x)] = c f'(x)\n",
        "$$\n",
        "\n",
        "* **Sum/Difference:**\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}[f(x) \\pm g(x)] = f'(x) \\pm g'(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Product Rule**\n",
        "\n",
        "If $h(x) = f(x) g(x)$:\n",
        "\n",
        "$$\n",
        "h'(x) = f'(x)g(x) + f(x)g'(x)\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "$\\frac{d}{dx}[x e^x] = 1 \\cdot e^x + x \\cdot e^x = (1+x)e^x$\n",
        "\n",
        "‚úîÔ∏è **ML Use:** Loss functions with multiple terms (e.g., $w \\cdot x$).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Quotient Rule**\n",
        "\n",
        "If $h(x) = \\frac{f(x)}{g(x)}$:\n",
        "\n",
        "$$\n",
        "h'(x) = \\frac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2}, \\quad g(x) \\neq 0\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "$\\frac{d}{dx}\\left(\\frac{x^2}{x+1}\\right) = \\frac{2x(x+1) - x^2(1)}{(x+1)^2} = \\frac{x^2+2x}{(x+1)^2}$\n",
        "\n",
        "‚úîÔ∏è **ML Use:** Regularization terms like $\\frac{1}{1+e^{-x}}$ (sigmoid).\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Chain Rule**\n",
        "\n",
        "If $y = f(g(x))$:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "$\\frac{d}{dx}[\\sin(x^2)] = \\cos(x^2) \\cdot 2x$\n",
        "\n",
        "‚úîÔ∏è **ML Use:** Backpropagation is just repeated application of chain rule!\n",
        "\n",
        "---\n",
        "\n",
        "## 6. **Derivatives of Common Functions**\n",
        "\n",
        "* $\\frac{d}{dx}[e^x] = e^x$\n",
        "* $\\frac{d}{dx}[\\ln x] = \\frac{1}{x}$\n",
        "* $\\frac{d}{dx}[\\sin x] = \\cos x$\n",
        "* $\\frac{d}{dx}[\\cos x] = -\\sin x$\n",
        "* $\\frac{d}{dx}[\\tanh x] = 1 - \\tanh^2 x$\n",
        "\n",
        "‚úîÔ∏è Sigmoid derivative:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}, \\quad \\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n",
        "$$\n",
        "\n",
        "(super important in neural nets).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick ML Connection\n",
        "\n",
        "| Rule                        | Use in ML                          |\n",
        "| --------------------------- | ---------------------------------- |\n",
        "| Product Rule                | Weight √ó input terms               |\n",
        "| Quotient Rule               | Softmax, sigmoid derivatives       |\n",
        "| Chain Rule                  | Backpropagation in deep learning   |\n",
        "| Power Rule                  | Polynomial loss, regularization    |\n",
        "| Exponential/Log derivatives | Logistic regression, cross-entropy |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# **3. Gradient = Vector of Partial Derivatives**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Partial Derivative**\n",
        "\n",
        "For a multivariable function $f(x_1, x_2, \\dots, x_n)$:\n",
        "\n",
        "The **partial derivative w\\.r.t. $x_i$** is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, ..., x_i+h, ..., x_n) - f(x_1, ..., x_i, ..., x_n)}{h}\n",
        "$$\n",
        "\n",
        "üëâ It measures how $f$ changes if only $x_i$ changes, keeping others fixed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "f(x,y) = x^2y + 3y\n",
        "$$\n",
        "\n",
        "* $\\frac{\\partial f}{\\partial x} = 2xy$\n",
        "* $\\frac{\\partial f}{\\partial y} = x^2 + 3$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Gradient**\n",
        "\n",
        "The **gradient** of $f(x_1, x_2, ..., x_n)$ is the **vector of partial derivatives**:\n",
        "\n",
        "$$\n",
        "\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n} \\right]^T\n",
        "$$\n",
        "\n",
        "üëâ Gradient points in the **direction of steepest increase** of the function.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "For $f(x,y) = x^2 + y^2$:\n",
        "\n",
        "$$\n",
        "\\nabla f(x,y) = [2x, \\; 2y]^T\n",
        "$$\n",
        "\n",
        "At point (1,1): $\\nabla f = [2, 2]$.\n",
        "\n",
        "Geometric meaning: function increases fastest in direction $[2,2]$.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Gradient in ML**\n",
        "\n",
        "* Loss function $L(w)$ depends on **parameters (weights)** $w$.\n",
        "* Gradient = tells us how to adjust each weight to minimize loss.\n",
        "\n",
        "**Gradient Descent Update Rule:**\n",
        "\n",
        "$$\n",
        "w := w - \\eta \\nabla L(w)\n",
        "$$\n",
        "\n",
        "where $\\eta$ = learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "### **ML Examples**\n",
        "\n",
        "* **Linear Regression**: Gradient w\\.r.t. weights gives Normal Equation or iterative updates.\n",
        "* **Logistic Regression**: Gradient involves sigmoid derivative.\n",
        "* **Neural Networks**: Gradients flow back using **backpropagation** (chain rule + gradients).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Hessian (Second Derivative Generalization)**\n",
        "\n",
        "* Hessian = matrix of second partial derivatives.\n",
        "* Tells about curvature (convex, concave, saddle).\n",
        "* Used in advanced optimization (Newton‚Äôs method).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick Summary\n",
        "\n",
        "| Concept            | Formula                                                                | ML Connection                 |\n",
        "| ------------------ | ---------------------------------------------------------------------- | ----------------------------- |\n",
        "| Partial derivative | $\\frac{\\partial f}{\\partial x_i}$                                      | Sensitivity to one variable   |\n",
        "| Gradient           | $\\nabla f = [\\partial f/\\partial x_1, \\dots, \\partial f/\\partial x_n]$ | Direction of steepest change  |\n",
        "| Gradient Descent   | $w := w - \\eta \\nabla L(w)$                                            | Training ML models            |\n",
        "| Hessian            | Matrix of 2nd derivatives                                              | Optimization, convexity check |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **4. Jacobian & Hessian**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Jacobian**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "For a vector-valued function\n",
        "\n",
        "$$\n",
        "\\mathbf{f}(x) =\n",
        "\\begin{bmatrix}\n",
        "f_1(x_1, \\dots, x_n) \\\\\n",
        "f_2(x_1, \\dots, x_n) \\\\\n",
        "\\vdots \\\\\n",
        "f_m(x_1, \\dots, x_n)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The **Jacobian matrix** is the matrix of first-order partial derivatives:\n",
        "\n",
        "$$\n",
        "J = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
        "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "\\mathbf{f}(x,y) =\n",
        "\\begin{bmatrix}\n",
        "f_1(x,y) = x^2y \\\\\n",
        "f_2(x,y) = \\sin(x) + y\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Jacobian:\n",
        "\n",
        "$$\n",
        "J =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
        "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "2xy & x^2 \\\\\n",
        "\\cos(x) & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **ML Connection**\n",
        "\n",
        "* Used in **backpropagation** (mapping gradients between layers).\n",
        "* **Change of variables** in probability (determinant of Jacobian for density transforms).\n",
        "* **Neural networks:** Jacobian tells how small input changes affect outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Hessian**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "For a scalar function $f(x_1, x_2, \\dots, x_n)$:\n",
        "\n",
        "The **Hessian matrix** is the matrix of second-order partial derivatives:\n",
        "\n",
        "$$\n",
        "H =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "üëâ Always square ($n \\times n$) and usually symmetric (if mixed partials are continuous).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "f(x,y) = x^2 + xy + y^2\n",
        "$$\n",
        "\n",
        "* $\\frac{\\partial^2 f}{\\partial x^2} = 2$\n",
        "* $\\frac{\\partial^2 f}{\\partial x \\partial y} = 1$\n",
        "* $\\frac{\\partial^2 f}{\\partial y^2} = 2$\n",
        "\n",
        "Hessian:\n",
        "\n",
        "$$\n",
        "H =\n",
        "\\begin{bmatrix}\n",
        "2 & 1 \\\\\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **ML Connection**\n",
        "\n",
        "* **Optimization:**\n",
        "\n",
        "  * If all eigenvalues of $H$ > 0 ‚Üí convex (local minimum).\n",
        "  * If all < 0 ‚Üí concave (local maximum).\n",
        "  * Mixed signs ‚Üí saddle point.\n",
        "* **Newton‚Äôs Method:** Uses Hessian to update parameters faster than gradient descent.\n",
        "* **Deep learning:** Checking curvature ‚Üí helps detect vanishing/exploding gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick Summary\n",
        "\n",
        "| Concept  | Definition                                           | ML Use                    |\n",
        "| -------- | ---------------------------------------------------- | ------------------------- |\n",
        "| Jacobian | Matrix of first-order partials (vector ‚Üí vector)     | Backprop, transformations |\n",
        "| Hessian  | Matrix of second-order partials (scalar ‚Üí curvature) | Optimization, convexity   |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **5. Taylor Series Approximation**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Idea**\n",
        "\n",
        "A **smooth function** can be approximated near a point by a polynomial using its derivatives.\n",
        "\n",
        "At $x = a$:\n",
        "\n",
        "$$\n",
        "f(x) \\approx f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\frac{f^{(3)}(a)}{3!}(x-a)^3 + \\dots\n",
        "$$\n",
        "\n",
        "üëâ This is called the **Taylor Series expansion** of $f(x)$ about $a$.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Special Case: Maclaurin Series (a = 0)**\n",
        "\n",
        "$$\n",
        "f(x) \\approx f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f^{(3)}(0)}{3!}x^3 + \\dots\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Examples**\n",
        "\n",
        "* $e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots$\n",
        "* $\\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\dots$\n",
        "* $\\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\dots$\n",
        "\n",
        "‚úîÔ∏è Neural nets often approximate nonlinearities using such expansions.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **First-Order Approximation (Linearization)**\n",
        "\n",
        "$$\n",
        "f(x) \\approx f(a) + f'(a)(x-a)\n",
        "$$\n",
        "\n",
        "üëâ This is just the **tangent line approximation** near $x=a$.\n",
        "Used in **gradient descent** (local linear approximation of loss).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Second-Order Approximation (Quadratic)**\n",
        "\n",
        "$$\n",
        "f(x) \\approx f(a) + f'(a)(x-a) + \\frac{1}{2}(x-a)^T H (x-a)\n",
        "$$\n",
        "\n",
        "(where $H$ = Hessian).\n",
        "\n",
        "üëâ This captures **curvature**, useful in **Newton‚Äôs method**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Error Term (Remainder)**\n",
        "\n",
        "The error in truncating the series after $n$ terms:\n",
        "\n",
        "$$\n",
        "R_n(x) = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\n",
        "$$\n",
        "\n",
        "for some $c$ between $a$ and $x$.\n",
        "\n",
        "‚úîÔ∏è Guarantees how close approximation is.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. **ML Intuition**\n",
        "\n",
        "* **Optimization:**\n",
        "\n",
        "  * Gradient descent ‚âà first-order Taylor expansion.\n",
        "  * Newton‚Äôs method ‚âà second-order expansion (uses Hessian).\n",
        "\n",
        "* **Activation Functions:**\n",
        "\n",
        "  * Sigmoid, tanh can be approximated using Taylor expansion for analysis.\n",
        "\n",
        "* **Kernel methods:**\n",
        "\n",
        "  * Some kernels are derived from expansions (e.g., polynomial kernels).\n",
        "\n",
        "* **Uncertainty:**\n",
        "\n",
        "  * Approximating log-likelihood functions in Bayesian inference.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick Summary\n",
        "\n",
        "| Approximation | Formula                                                       | ML Use                 |\n",
        "| ------------- | ------------------------------------------------------------- | ---------------------- |\n",
        "| 1st order     | $f(x) \\approx f(a) + f'(a)(x-a)$                              | Gradient descent       |\n",
        "| 2nd order     | $f(x) \\approx f(a) + f'(a)(x-a) + \\frac{1}{2}(x-a)^T H (x-a)$ | Newton‚Äôs method        |\n",
        "| Higher order  | Add cubic, quartic‚Ä¶ terms                                     | Function approximation |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **6. Integration Basics**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **What is Integration?**\n",
        "\n",
        "* **Differentiation** = rate of change (slope).\n",
        "* **Integration** = accumulation (area under a curve).\n",
        "\n",
        "üëâ Think of integration as the **inverse of differentiation**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Indefinite Integral (Antiderivative)**\n",
        "\n",
        "The **indefinite integral** of $f(x)$:\n",
        "\n",
        "$$\n",
        "\\int f(x)\\, dx = F(x) + C\n",
        "$$\n",
        "\n",
        "where $F'(x) = f(x)$, and $C$ = constant of integration.\n",
        "\n",
        "---\n",
        "\n",
        "### **Basic Rules**\n",
        "\n",
        "* Power rule: $\\int x^n dx = \\frac{x^{n+1}}{n+1} + C, \\quad (n \\neq -1)$\n",
        "* Constant multiple: $\\int c f(x) dx = c \\int f(x) dx$\n",
        "* Sum rule: $\\int (f+g) dx = \\int f dx + \\int g dx$\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Integrals**\n",
        "\n",
        "* $\\int e^x dx = e^x + C$\n",
        "* $\\int \\frac{1}{x} dx = \\ln |x| + C$\n",
        "* $\\int \\sin x dx = -\\cos x + C$\n",
        "* $\\int \\cos x dx = \\sin x + C$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Definite Integral (Area Under Curve)**\n",
        "\n",
        "$$\n",
        "\\int_a^b f(x)\\, dx = F(b) - F(a)\n",
        "$$\n",
        "\n",
        "üëâ Represents the **net area under $f(x)$** between $x=a$ and $x=b$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "\\int_0^1 x^2 dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Fundamental Theorem of Calculus**\n",
        "\n",
        "Connects derivatives and integrals:\n",
        "\n",
        "1. Differentiation undoes integration.\n",
        "\n",
        "   $$\n",
        "   \\frac{d}{dx} \\left( \\int_a^x f(t) dt \\right) = f(x)\n",
        "   $$\n",
        "\n",
        "2. Integration accumulates derivatives:\n",
        "\n",
        "   $$\n",
        "   \\int_a^b f(x) dx = F(b) - F(a)\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **ML Applications**\n",
        "\n",
        "* **Probability:**\n",
        "\n",
        "  * Continuous distributions ‚Üí area under density = 1.\n",
        "  * Example: $\\int_{-\\infty}^\\infty p(x) dx = 1$.\n",
        "\n",
        "* **Expectation & Variance:**\n",
        "\n",
        "  * $E[X] = \\int x p(x) dx$.\n",
        "  * $Var(X) = \\int (x-\\mu)^2 p(x) dx$.\n",
        "\n",
        "* **Neural nets:** Softmax normalization involves integrals in continuous analogues.\n",
        "\n",
        "* **Partition functions:** In probabilistic models (e.g., Boltzmann machines).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick Summary\n",
        "\n",
        "| Concept             | Formula                                  | Meaning                 |\n",
        "| ------------------- | ---------------------------------------- | ----------------------- |\n",
        "| Indefinite Integral | $\\int f(x) dx = F(x) + C$                | General antiderivative  |\n",
        "| Definite Integral   | $\\int_a^b f(x) dx = F(b) - F(a)$         | Area under curve        |\n",
        "| Fundamental Theorem | $\\frac{d}{dx}\\int f = f(x)$              | Derivatives ‚Üî Integrals |\n",
        "| ML Use              | Probability, expectations, normalization | Ensures valid models    |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **7. Multivariable Calculus: Gradient, Divergence, Curl**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Gradient**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "For a scalar function $f(x,y,z)$:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right]^T\n",
        "$$\n",
        "\n",
        "üëâ The **gradient vector** points in the **direction of steepest increase** of $f$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "f(x,y) = x^2 + y^2 \\quad \\Rightarrow \\quad \\nabla f = [2x, \\; 2y]\n",
        "$$\n",
        "\n",
        "At (1,1): gradient = \\[2,2], pointing outward.\n",
        "\n",
        "‚úîÔ∏è **ML Use:** Gradient = backbone of **gradient descent** for loss minimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Divergence**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "For a vector field $\\mathbf{F} = [F_x, F_y, F_z]$:\n",
        "\n",
        "$$\n",
        "\\nabla \\cdot \\mathbf{F} = \\frac{\\partial F_x}{\\partial x} + \\frac{\\partial F_y}{\\partial y} + \\frac{\\partial F_z}{\\partial z}\n",
        "$$\n",
        "\n",
        "üëâ Measures the **‚Äúnet flow out‚Äù** of a point (like sources/sinks).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "\\mathbf{F} = [x, y, z] \\quad \\Rightarrow \\quad \\nabla \\cdot \\mathbf{F} = 1+1+1 = 3\n",
        "$$\n",
        "\n",
        "‚úîÔ∏è **ML Use:** Appears in **divergence measures** (e.g., KL Divergence conceptually tied to flow/expansion).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Curl**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "For a vector field $\\mathbf{F} = [F_x, F_y, F_z]$:\n",
        "\n",
        "$$\n",
        "\\nabla \\times \\mathbf{F} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial F_z}{\\partial y} - \\frac{\\partial F_y}{\\partial z} \\\\\n",
        "\\frac{\\partial F_x}{\\partial z} - \\frac{\\partial F_z}{\\partial x} \\\\\n",
        "\\frac{\\partial F_y}{\\partial x} - \\frac{\\partial F_x}{\\partial y}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "üëâ Measures **rotation** or swirling strength of a vector field.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "\\mathbf{F} = [-y, x, 0] \\quad \\Rightarrow \\quad \\nabla \\times \\mathbf{F} = [0, 0, 2]\n",
        "$$\n",
        "\n",
        "This represents a counter-clockwise rotation in the xy-plane.\n",
        "\n",
        "‚úîÔ∏è **ML Use:**\n",
        "\n",
        "* Physics-inspired ML (fluid simulations, robotics).\n",
        "* Vector field learning (e.g., diffusion models).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Big Picture**\n",
        "\n",
        "* **Gradient ($\\nabla f$)** ‚Üí slope (steepest ascent).\n",
        "* **Divergence ($\\nabla \\cdot F$)** ‚Üí expansion/contraction of vector fields.\n",
        "* **Curl ($\\nabla \\times F$)** ‚Üí rotation of vector fields.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Quick Summary\n",
        "\n",
        "| Concept    | Formula           | Meaning                           | ML Connection                      |\n",
        "| ---------- | ----------------- | --------------------------------- | ---------------------------------- |\n",
        "| Gradient   | $\\nabla f$        | Steepest increase of scalar field | Gradient descent                   |\n",
        "| Divergence | $\\nabla \\cdot F$  | Net outflow at a point            | Info flow, KL divergence analogy   |\n",
        "| Curl       | $\\nabla \\times F$ | Local rotation of field           | Physics-inspired ML, vector fields |\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "yf9wDTtO0KHz"
      }
    }
  ]
}