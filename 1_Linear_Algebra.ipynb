{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Scalars, Vectors, Matrices, Tensors**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Scalar**\n",
        "\n",
        "* **Definition:** A single number (real or complex).\n",
        "* **Dimension:** 0D (just a value).\n",
        "* **Examples:**\n",
        "\n",
        "  * Temperature = 37°C\n",
        "  * Learning rate $\\eta = 0.01$\n",
        "* **Notation:** Usually lowercase letters (e.g., $a, x, y$).\n",
        "\n",
        "✔️ Think of a scalar as just one point on the number line.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Vector**\n",
        "\n",
        "* **Definition:** An **ordered list of numbers** (1D array). Represents magnitude + direction.\n",
        "* **Dimension:** 1D\n",
        "* **Examples:**\n",
        "\n",
        "  * Position in 3D space: $v = [x, y, z]$\n",
        "  * Feature vector: $[height, weight, age]$\n",
        "* **Notation:** Bold lowercase letters ($\\mathbf{v}$), or with arrow ($\\vec{v}$).\n",
        "* **Operations:**\n",
        "\n",
        "  * Addition: $[1,2] + [3,4] = [4,6]$\n",
        "  * Dot product: $\\mathbf{a} \\cdot \\mathbf{b} = \\sum a_i b_i$\n",
        "\n",
        "✔️ Think of a vector as a **list of features** or a point in space.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Matrix**\n",
        "\n",
        "* **Definition:** A **2D array** of numbers arranged in rows and columns.\n",
        "* **Dimension:** 2D\n",
        "* **Examples:**\n",
        "\n",
        "  * $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$\n",
        "  * Image in grayscale = matrix of pixel values.\n",
        "* **Notation:** Bold uppercase letters ($\\mathbf{A}, \\mathbf{B}$).\n",
        "* **Operations:**\n",
        "\n",
        "  * Matrix Addition: Add element-wise.\n",
        "  * Matrix Multiplication: Row × Column rule.\n",
        "  * Transpose: Flip rows ↔ columns.\n",
        "\n",
        "✔️ Think of a matrix as a **table of values** or **multiple vectors stacked**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Tensor**\n",
        "\n",
        "* **Definition:** A **generalization of scalars, vectors, and matrices** to higher dimensions.\n",
        "* **Dimension:** nD (multi-dimensional arrays).\n",
        "* **Examples:**\n",
        "\n",
        "  * Scalar → 0D Tensor\n",
        "  * Vector → 1D Tensor\n",
        "  * Matrix → 2D Tensor\n",
        "  * Color Image → 3D Tensor (Height × Width × Channels)\n",
        "  * Batch of Images → 4D Tensor (Batch × Height × Width × Channels)\n",
        "* **Notation:** Bold script letters or calligraphic letters ($\\mathcal{T}$).\n",
        "\n",
        "✔️ Think of a tensor as a **container of numbers with multiple axes (dimensions)**.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Visualization\n",
        "\n",
        "| Object | Dimension | Example                                        | ML Use Case                  |\n",
        "| ------ | --------- | ---------------------------------------------- | ---------------------------- |\n",
        "| Scalar | 0D        | $a = 5$                                        | Learning rate, loss value    |\n",
        "| Vector | 1D        | $[1, 2, 3]$                                    | Feature vector               |\n",
        "| Matrix | 2D        | $\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$ | Weight matrix in NN          |\n",
        "| Tensor | nD        | Image = $(64, 64, 3)$                          | Data representation in ML/DL |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **2. Matrix Operations**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Matrix Addition**\n",
        "\n",
        "* **Rule:** Add corresponding elements (same dimensions required).\n",
        "* If $A$ and $B$ are both $m \\times n$, then:\n",
        "\n",
        "  $$\n",
        "  (A + B)_{ij} = A_{ij} + B_{ij}\n",
        "  $$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad\n",
        "B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A + B = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "✔️ Simple element-wise operation.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Matrix Multiplication**\n",
        "\n",
        "Two types: **Scalar × Matrix** and **Matrix × Matrix**\n",
        "\n",
        "### (a) Scalar Multiplication\n",
        "\n",
        "Multiply each entry by a scalar $k$.\n",
        "\n",
        "$$\n",
        "kA = \\begin{bmatrix} ka_{11} & ka_{12} \\\\ ka_{21} & ka_{22} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "\n",
        "$$\n",
        "2 \\times \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
        "= \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### (b) Matrix × Matrix Multiplication\n",
        "\n",
        "* **Rule:** Row of 1st × Column of 2nd.\n",
        "* If $A$ is $m \\times n$, $B$ must be $n \\times p$.\n",
        "* Result = $m \\times p$.\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "(A \\times B)_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad\n",
        "B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A \\times B =\n",
        "\\begin{bmatrix}\n",
        "(1\\cdot5 + 2\\cdot7) & (1\\cdot6 + 2\\cdot8) \\\\\n",
        "(3\\cdot5 + 4\\cdot7) & (3\\cdot6 + 4\\cdot8)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "✔️ Order matters: $A \\times B \\neq B \\times A$ in general.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Matrix Transpose**\n",
        "\n",
        "* **Rule:** Flip over diagonal → rows become columns.\n",
        "* If $A$ is $m \\times n$, then $A^T$ is $n \\times m$.\n",
        "\n",
        "$$\n",
        "(A^T)_{ij} = A_{ji}\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "✔️ Useful in dot products, orthogonality, and linear transformations.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Quick Summary\n",
        "\n",
        "* **Addition:** Same size → add element-wise.\n",
        "* **Multiplication:** Row × Column (dimensions must match).\n",
        "* **Transpose:** Flip rows ↔ columns.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **3. Identity & Inverse Matrices**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Identity Matrix** ($I$)\n",
        "\n",
        "* **Definition:** A **square matrix** with **1’s on the diagonal** and **0’s elsewhere**.\n",
        "* Acts like **“1” for matrices** under multiplication.\n",
        "* Size: $n \\times n$.\n",
        "\n",
        "$$\n",
        "I = \\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Property:**\n",
        "\n",
        "$$\n",
        "AI = IA = A\n",
        "$$\n",
        "\n",
        "✔️ Think of it as the **neutral element** in matrix multiplication.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Inverse Matrix** ($A^{-1}$)\n",
        "\n",
        "* **Definition:** For a square matrix $A$, its inverse $A^{-1}$ is defined such that:\n",
        "\n",
        "$$\n",
        "A A^{-1} = A^{-1} A = I\n",
        "$$\n",
        "\n",
        "* **Conditions:**\n",
        "\n",
        "  1. $A$ must be **square** ($n \\times n$).\n",
        "  2. $A$ must be **non-singular** (determinant ≠ 0).\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Compute Inverse (2×2 Case)**\n",
        "\n",
        "If\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "then\n",
        "\n",
        "$$\n",
        "A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "where $ad - bc = \\det(A)$.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 2 & 1 \\\\ 5 & 3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\det(A) = (2)(3) - (5)(1) = 6 - 5 = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "A^{-1} = \\begin{bmatrix} 3 & -1 \\\\ -5 & 2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Properties of Inverses**\n",
        "\n",
        "* $(A^{-1})^{-1} = A$\n",
        "* $(AB)^{-1} = B^{-1}A^{-1}$ (note the **reverse order**)\n",
        "* $(A^T)^{-1} = (A^{-1})^T$\n",
        "* If $\\det(A) = 0$ → **No inverse** (singular matrix).\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Why Important in ML?\n",
        "\n",
        "* In **Linear Regression**:\n",
        "  Solution of $y = X\\beta$ is:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\beta} = (X^T X)^{-1} X^T y\n",
        "  $$\n",
        "* In **PCA**: Eigen decomposition uses inverses.\n",
        "* In **Optimization**: Newton’s method uses Hessian inverse.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **4. Determinant & Rank of a Matrix**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Determinant** ($\\det(A)$ or $|A|$)\n",
        "\n",
        "* **Definition:** A scalar value that represents the **scaling factor** of a matrix transformation.\n",
        "* Defined only for **square matrices** ($n \\times n$).\n",
        "\n",
        "---\n",
        "\n",
        "### **Geometric Meaning**\n",
        "\n",
        "* $|A|$ = area (2D) or volume (3D) scaling factor after applying matrix $A$.\n",
        "* If $\\det(A) = 0$ → transformation **collapses space** (loses dimension).\n",
        "\n",
        "---\n",
        "\n",
        "### **Computation**\n",
        "\n",
        "* **2×2 Matrix:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad \\det(A) = ad - bc\n",
        "$$\n",
        "\n",
        "* **3×3 Matrix:**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Properties**\n",
        "\n",
        "* $\\det(AB) = \\det(A) \\cdot \\det(B)$\n",
        "* $\\det(A^T) = \\det(A)$\n",
        "* If $\\det(A) = 0$, matrix is **singular** (no inverse).\n",
        "\n",
        "---\n",
        "\n",
        "✔️ **ML Use:**\n",
        "\n",
        "* Inverse exists iff $\\det(A) \\neq 0$.\n",
        "* Covariance matrix determinant → volume of distribution spread.\n",
        "* Jacobian determinant → change of variables in probability (used in normalizing flows).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Rank of a Matrix**\n",
        "\n",
        "* **Definition:** Number of **linearly independent rows or columns** in a matrix.\n",
        "* Rank tells how much **useful information** a matrix has.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation**\n",
        "\n",
        "* **Full Rank:** Rank = min(rows, cols). → No redundancy.\n",
        "* **Rank Deficient:** Rank < min(rows, cols). → Some rows/cols are linear combinations of others.\n",
        "\n",
        "---\n",
        "\n",
        "### **Examples**\n",
        "\n",
        "1.\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Here row2 = 3×row1 → **Rank = 1** (not full rank).\n",
        "\n",
        "2.\n",
        "\n",
        "$$\n",
        "B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Rank = 2 (full rank).\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Find Rank**\n",
        "\n",
        "* Reduce matrix to **Row Echelon Form (REF)** or **Reduced Row Echelon Form (RREF)** → count nonzero rows.\n",
        "* Alternatively → number of non-zero singular values (from SVD).\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Facts**\n",
        "\n",
        "* $\\text{rank}(A) \\leq \\min(m, n)$.\n",
        "* If rank < n (columns), then system $Ax=b$ has **no unique solution**.\n",
        "* Rank is crucial in **linear regression, PCA, and dimensionality reduction**.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Summary\n",
        "\n",
        "* **Determinant:** Scalar that shows scaling + invertibility.\n",
        "* **Rank:** Number of independent vectors (information capacity).\n",
        "* If $\\det(A) = 0 \\Rightarrow \\text{rank}(A) < n$.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# **5. Dot Product & Cross Product**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Dot Product** ($\\mathbf{a} \\cdot \\mathbf{b}$)\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "* A scalar value (not a vector).\n",
        "* Measures **similarity** between two vectors.\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\n",
        "$$\n",
        "\n",
        "or geometrically:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos\\theta\n",
        "$$\n",
        "\n",
        "where $\\theta$ is the angle between vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = [1, 2, 3], \\quad \\mathbf{b} = [4, 5, 6]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = 1\\cdot4 + 2\\cdot5 + 3\\cdot6 = 32\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Properties:**\n",
        "\n",
        "* $\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{b} \\cdot \\mathbf{a}$ (commutative).\n",
        "* $\\mathbf{a} \\cdot \\mathbf{a} = \\|\\mathbf{a}\\|^2$.\n",
        "* If $\\mathbf{a} \\cdot \\mathbf{b} = 0 \\Rightarrow \\mathbf{a}, \\mathbf{b}$ are **orthogonal**.\n",
        "\n",
        "---\n",
        "\n",
        "✔️ **ML Use Cases:**\n",
        "\n",
        "* Cosine Similarity:\n",
        "  $\\cos\\theta = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}$ (used in NLP, embeddings).\n",
        "* Loss functions & projections.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Cross Product** ($\\mathbf{a} \\times \\mathbf{b}$)\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "* A vector (not a scalar).\n",
        "* Only defined in **3D space**.\n",
        "* Result is **perpendicular** to both $\\mathbf{a}, \\mathbf{b}$.\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\times \\mathbf{b} =\n",
        "\\begin{vmatrix}\n",
        "\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
        "a_1 & a_2 & a_3 \\\\\n",
        "b_1 & b_2 & b_3\n",
        "\\end{vmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (a_2b_3 - a_3b_2)\\mathbf{i} - (a_1b_3 - a_3b_1)\\mathbf{j} + (a_1b_2 - a_2b_1)\\mathbf{k}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = [1, 2, 3], \\quad \\mathbf{b} = [4, 5, 6]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\times \\mathbf{b} =\n",
        "\\begin{bmatrix} (2\\cdot6 - 3\\cdot5), & (3\\cdot4 - 1\\cdot6), & (1\\cdot5 - 2\\cdot4) \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [-3, 6, -3]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Geometric Meaning:**\n",
        "\n",
        "* Magnitude = area of parallelogram formed by $\\mathbf{a}, \\mathbf{b}$.\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{a} \\times \\mathbf{b}\\| = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\sin\\theta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Properties:**\n",
        "\n",
        "* $\\mathbf{a} \\times \\mathbf{b} = -(\\mathbf{b} \\times \\mathbf{a})$.\n",
        "* $\\mathbf{a} \\times \\mathbf{a} = \\mathbf{0}$.\n",
        "* Result is **orthogonal** to both vectors.\n",
        "\n",
        "---\n",
        "\n",
        "✔️ **ML Use Cases:**\n",
        "\n",
        "* Less common than dot product in ML, but appears in **3D computer vision, graphics, robotics**.\n",
        "* Useful in calculating **normal vectors** for 3D planes.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Summary\n",
        "\n",
        "| Operation     | Result | Dimension | Key Use Case                               |\n",
        "| ------------- | ------ | --------- | ------------------------------------------ |\n",
        "| Dot Product   | Scalar | Any nD    | Similarity, projections, embeddings        |\n",
        "| Cross Product | Vector | Only 3D   | Geometry, 3D ML/vision, orthogonal vectors |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# **6. Vector Norms (L1, L2, ∞ Norm)**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **What is a Norm?**\n",
        "\n",
        "* A **function** that assigns a non-negative length (magnitude) to a vector.\n",
        "* Must satisfy:\n",
        "\n",
        "  1. $\\|\\mathbf{x}\\| \\geq 0$ and $\\|\\mathbf{x}\\| = 0 \\iff \\mathbf{x} = 0$\n",
        "  2. $\\|c \\mathbf{x}\\| = |c|\\|\\mathbf{x}\\|$ (scaling property)\n",
        "  3. $\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|$ (triangle inequality)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **L1 Norm (Manhattan Norm / Taxicab Norm)**\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n |x_i|\n",
        "$$\n",
        "\n",
        "* Distance = “grid-like path” (like moving on city streets).\n",
        "* Encourages **sparsity** (many coefficients become 0 in ML).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = [3, -4, 5]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_1 = |3| + |-4| + |5| = 12\n",
        "$$\n",
        "\n",
        "✔️ **ML Use:** Lasso Regression (L1 regularization).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **L2 Norm (Euclidean Norm)**\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n",
        "$$\n",
        "\n",
        "* Standard **Euclidean distance** (straight-line).\n",
        "* Smooth, differentiable → good for optimization.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = [3, -4, 5]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_2 = \\sqrt{3^2 + (-4)^2 + 5^2} = \\sqrt{50} \\approx 7.07\n",
        "$$\n",
        "\n",
        "✔️ **ML Use:** Ridge Regression (L2 regularization), Gradient Descent step sizes.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Infinity Norm (Max Norm, $L_\\infty$)**\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_\\infty = \\max_i |x_i|\n",
        "$$\n",
        "\n",
        "* Largest absolute value among vector elements.\n",
        "* Focuses on the “worst-case” component.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = [3, -4, 5]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_\\infty = \\max(3, 4, 5) = 5\n",
        "$$\n",
        "\n",
        "✔️ **ML Use:** Robust optimization, adversarial ML (bounding perturbations).\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Comparison of Norms**\n",
        "\n",
        "For $\\mathbf{x} = [3, -4, 5]$:\n",
        "\n",
        "| Norm       | Formula                     | Value |   |    |   |   |    |    |\n",
        "| ---------- | --------------------------- | ----- | - | -- | - | - | -- | -- |\n",
        "| L1         | (                           | 3     | + | -4 | + | 5 | )  | 12 |\n",
        "| L2         | $\\sqrt{3^2 + (-4)^2 + 5^2}$ | 7.07  |   |    |   |   |    |    |\n",
        "| $L_\\infty$ | (\\max(                      | 3     | , | -4 | , | 5 | )) | 5  |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ ML Intuition\n",
        "\n",
        "* **L1** → Sparsity (feature selection).\n",
        "* **L2** → Stability, smooth solutions.\n",
        "* **$L_\\infty$** → Robustness against max perturbation.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **7. Linear Independence, Basis, Dimension**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Linear Independence**\n",
        "\n",
        "* A set of vectors $\\{v_1, v_2, ..., v_n\\}$ is **linearly independent** if **no vector can be written as a linear combination of others**.\n",
        "* Otherwise, they are **linearly dependent**.\n",
        "\n",
        "**Formally:**\n",
        "\n",
        "$$\n",
        "c_1v_1 + c_2v_2 + \\dots + c_nv_n = 0 \\quad \\Rightarrow \\quad c_1 = c_2 = \\dots = c_n = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Example (Independent):**\n",
        "\n",
        "$$\n",
        "v_1 = [1, 0], \\quad v_2 = [0, 1]\n",
        "$$\n",
        "\n",
        "No way to express one using the other → Independent.\n",
        "\n",
        "**Example (Dependent):**\n",
        "\n",
        "$$\n",
        "v_1 = [1, 2], \\quad v_2 = [2, 4]\n",
        "$$\n",
        "\n",
        "Here $v_2 = 2v_1$ → Dependent.\n",
        "\n",
        "✔️ **ML Use:** Linear independence means features contain **unique information**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Basis of a Vector Space**\n",
        "\n",
        "* A **basis** is a **set of linearly independent vectors** that can represent **all vectors in the space** via linear combinations.\n",
        "* Basis vectors are like the **\"coordinate system\"** for the space.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Standard basis of $\\mathbb{R}^2$:\n",
        "\n",
        "$$\n",
        "e_1 = [1, 0], \\quad e_2 = [0, 1]\n",
        "$$\n",
        "\n",
        "Any vector $[x, y]$ can be written as $x e_1 + y e_2$.\n",
        "\n",
        "✔️ **Different bases** can represent the same space (e.g., rotated axes).\n",
        "\n",
        "✔️ **ML Use:** PCA finds a **new basis** (principal components) that explains variance.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Dimension of a Vector Space**\n",
        "\n",
        "* The **dimension** of a space = number of vectors in its **basis**.\n",
        "* It represents the **degrees of freedom** or amount of independent information.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Line ($\\mathbb{R}^1$) → Dimension = 1\n",
        "* Plane ($\\mathbb{R}^2$) → Dimension = 2\n",
        "* 3D Space ($\\mathbb{R}^3$) → Dimension = 3\n",
        "\n",
        "✔️ **ML Use:**\n",
        "\n",
        "* Dimensionality = number of features.\n",
        "* Dimensionality reduction (PCA, autoencoders) → reduce redundancy.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Quick Summary\n",
        "\n",
        "| Concept             | Meaning                                            | ML Connection                     |\n",
        "| ------------------- | -------------------------------------------------- | --------------------------------- |\n",
        "| Linear Independence | Vectors don’t overlap in information               | Ensures features aren’t redundant |\n",
        "| Basis               | Minimum set of independent vectors that span space | PCA basis, embeddings             |\n",
        "| Dimension           | Number of vectors in basis                         | Feature space size                |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **8. Eigenvalues & Eigenvectors**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Definition**\n",
        "\n",
        "For a square matrix $A$:\n",
        "\n",
        "$$\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "* $\\mathbf{v}$ = **Eigenvector** (non-zero vector).\n",
        "* $\\lambda$ = **Eigenvalue** (scalar).\n",
        "\n",
        "👉 Interpretation: Applying $A$ to $\\mathbf{v}$ just **scales it**, not change its direction.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **How to Find Them**\n",
        "\n",
        "* From equation:\n",
        "\n",
        "$$\n",
        "(A - \\lambda I)\\mathbf{v} = 0\n",
        "$$\n",
        "\n",
        "* Non-trivial solution exists if:\n",
        "\n",
        "$$\n",
        "\\det(A - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "This gives the **characteristic polynomial**, solving it → eigenvalues $\\lambda$.\n",
        "\n",
        "* Substitute $\\lambda$ back to solve for $\\mathbf{v}$.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Example (2×2 Case)**\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Characteristic equation:\n",
        "\n",
        "$$\n",
        "\\det(A - \\lambda I) =\n",
        "\\begin{vmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{vmatrix}\n",
        "= (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3\n",
        "$$\n",
        "\n",
        "Solve: $\\lambda^2 - 4\\lambda + 3 = 0 \\Rightarrow \\lambda = 1, 3$.\n",
        "\n",
        "For $\\lambda = 3$:\n",
        "\n",
        "$$\n",
        "(A - 3I)v = 0 \\Rightarrow \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} v = 0\n",
        "$$\n",
        "\n",
        "Eigenvector: $[1, 1]^T$.\n",
        "\n",
        "For $\\lambda = 1$:\n",
        "Eigenvector: $[1, -1]^T$.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Geometric Meaning**\n",
        "\n",
        "* Eigenvectors = **special directions** that remain unchanged (up to scaling) under transformation.\n",
        "* Eigenvalues = scaling factors along those directions.\n",
        "\n",
        "✔️ Example: Stretching a rubber sheet → eigenvectors = directions of stretch, eigenvalues = stretch amount.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Key Properties**\n",
        "\n",
        "* A matrix has at most $n$ eigenvalues (for $n \\times n$).\n",
        "* $\\det(A) = \\prod \\lambda_i$.\n",
        "* $\\text{trace}(A) = \\sum \\lambda_i$.\n",
        "* If all eigenvalues > 0 → matrix is **positive definite** (important in optimization).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. **ML Use Cases**\n",
        "\n",
        "### 🔹 **Principal Component Analysis (PCA)**\n",
        "\n",
        "* Covariance matrix $C = \\frac{1}{n}X^TX$.\n",
        "* Eigenvectors of $C$ = **principal directions** (new feature axes).\n",
        "* Eigenvalues = variance explained by each principal component.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Optimization (Convexity & Curvature)**\n",
        "\n",
        "* Hessian matrix $H$ of second derivatives:\n",
        "\n",
        "  * If all eigenvalues > 0 → convex (local min).\n",
        "  * If all eigenvalues < 0 → concave (local max).\n",
        "  * Mixed signs → saddle point.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **Other Uses**\n",
        "\n",
        "* Spectral clustering (graph Laplacians).\n",
        "* PageRank (Google) uses eigenvector centrality.\n",
        "* Deep learning stability → weight matrix eigenvalues control gradient explosion/vanishing.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Summary\n",
        "\n",
        "| Concept          | Meaning                               | ML Connection                              |\n",
        "| ---------------- | ------------------------------------- | ------------------------------------------ |\n",
        "| Eigenvector      | Special direction preserved by matrix | PCA axes, graph embeddings                 |\n",
        "| Eigenvalue       | Scaling factor along eigenvector      | Variance explained, curvature strength     |\n",
        "| Large eigenvalue | Strong effect in that direction       | Dominant feature/component                 |\n",
        "| Small eigenvalue | Weak/flat direction                   | Can be removed in dimensionality reduction |\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **9. Orthogonality & Projections**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Orthogonality**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "* Two vectors $\\mathbf{a}, \\mathbf{b}$ are **orthogonal** if:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = 0\n",
        "$$\n",
        "\n",
        "* Means they are **perpendicular** in space.\n",
        "\n",
        "---\n",
        "\n",
        "### **Properties**\n",
        "\n",
        "* Orthogonal vectors are **linearly independent**.\n",
        "* An **orthogonal basis** = set of mutually perpendicular vectors.\n",
        "* If all basis vectors are unit length → **orthonormal basis**.\n",
        "\n",
        "---\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = [1, 0], \\quad \\mathbf{b} = [0, 1]\n",
        "$$\n",
        "\n",
        "$\\mathbf{a} \\cdot \\mathbf{b} = 0 $ → orthogonal.\n",
        "\n",
        "✔️ **ML Use:** Word embeddings, PCA (principal components are orthogonal).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Projection of a Vector onto Another**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Projection of vector $\\mathbf{a}$ onto vector $\\mathbf{b}$:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{\\mathbf{b}}(\\mathbf{a}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2} \\mathbf{b}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Geometric Meaning**\n",
        "\n",
        "* It’s the **shadow** of $\\mathbf{a}$ onto the direction of $\\mathbf{b}$.\n",
        "* Decomposes $\\mathbf{a}$ into:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{a} = \\text{proj}_{\\mathbf{b}}(\\mathbf{a}) + \\mathbf{a}_\\perp\n",
        "  $$\n",
        "\n",
        "  where $\\mathbf{a}_\\perp$ is orthogonal to $\\mathbf{b}$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = [3, 4], \\quad \\mathbf{b} = [1, 0]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{proj}_{\\mathbf{b}}(\\mathbf{a}) = \\frac{(3)(1) + (4)(0)}{1^2 + 0^2}[1, 0] = [3, 0]\n",
        "$$\n",
        "\n",
        "So $[3,4]$ is decomposed into $[3,0]$ (parallel to $\\mathbf{b}$) and $[0,4]$ (orthogonal).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Projections onto Subspaces (Generalized)**\n",
        "\n",
        "* For a matrix $X$ with columns as basis vectors, the projection of vector $y$ onto the subspace spanned by $X$ is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X(X^TX)^{-1}X^T y\n",
        "$$\n",
        "\n",
        "✔️ This is exactly the formula used in **Least Squares Regression** (fitting line/plane to data).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **ML Use Cases**\n",
        "\n",
        "* **PCA** → Projects data onto eigenvector directions (principal components).\n",
        "* **Least Squares Regression** → Projects target $y$ onto the column space of $X$.\n",
        "* **Embeddings** → Orthogonal vectors = uncorrelated features.\n",
        "* **Orthogonalization** → Gram-Schmidt used to build orthogonal basis.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Quick Summary\n",
        "\n",
        "| Concept                        | Formula                                                         | ML Connection                |\n",
        "| ------------------------------ | --------------------------------------------------------------- | ---------------------------- |\n",
        "| Orthogonality                  | $\\mathbf{a} \\cdot \\mathbf{b} = 0$                               | Ensures independent features |\n",
        "| Projection (onto $\\mathbf{b}$) | $\\frac{\\mathbf{a}\\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2}\\mathbf{b}$ | Used in regression, PCA      |\n",
        "| Orthonormal basis              | Unit-length orthogonal vectors                                  | PCA, embeddings              |\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "e0tjG7VkvWjO"
      }
    }
  ]
}