{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic 1: Scalars, Vectors, Matrices, Tensors**\n",
        "---\n",
        "\n",
        "### 1. **Scalars**\n",
        "\n",
        "* Just a single number.\n",
        "* Examples in AI:\n",
        "\n",
        "  * **Learning rate** = 0.01\n",
        "  * **Bias term** in a neural network = 3\n",
        "\n",
        "üëâ Think of it as the smallest building block.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Vectors**\n",
        "\n",
        "* 1D array of numbers.\n",
        "* Represent **features** of a single data point.\n",
        "\n",
        "üìå Example: Student data ‚Üí `[age, height, weight] = [20, 170, 65]`\n",
        "\n",
        "üìå In NLP: word embeddings are vectors ‚Üí `\"king\" ‚âà [0.2, 0.7, -0.5, ...]`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Matrices**\n",
        "\n",
        "* 2D array (rows √ó columns).\n",
        "* Represent a dataset or transformations.\n",
        "\n",
        "üìå Example:\n",
        "If we have 3 students with 3 features each:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "20 & 170 & 65 \\\\\n",
        "22 & 180 & 70 \\\\\n",
        "19 & 160 & 55\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* Each **row** = one student\n",
        "* Each **column** = one feature\n",
        "\n",
        "üìå In images: a **grayscale image** is just a matrix of pixel values.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Tensors**\n",
        "\n",
        "* Generalization of vectors & matrices to higher dimensions.\n",
        "* 3D tensor = multiple matrices stacked (like a cube).\n",
        "\n",
        "üìå Example:\n",
        "\n",
        "* RGB Image ‚Üí Height √ó Width √ó 3 (channels).\n",
        "* In deep learning, a **batch of images** = 4D tensor ‚Üí (batch\\_size √ó height √ó width √ó channels).\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Python Practice\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Scalar\n",
        "scalar = 3.14\n",
        "print(\"Scalar:\", scalar)\n",
        "\n",
        "# Vector (1D)\n",
        "vector = np.array([20, 170, 65])  \n",
        "print(\"Vector:\", vector)\n",
        "\n",
        "# Matrix (2D)\n",
        "matrix = np.array([\n",
        "    [20, 170, 65],\n",
        "    [22, 180, 70],\n",
        "    [19, 160, 55]\n",
        "])\n",
        "print(\"Matrix:\\n\", matrix)\n",
        "\n",
        "# Tensor (3D) - random RGB image (2x2 pixels, 3 channels)\n",
        "tensor = np.random.randint(0, 256, (2, 2, 3))\n",
        "print(\"Tensor (RGB image example):\\n\", tensor)\n",
        "print(\"Tensor shape:\", tensor.shape)\n",
        "```\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 2: Vector Operations**\n",
        "---\n",
        "\n",
        "### 1. **Addition & Subtraction**\n",
        "\n",
        "* Add or subtract vectors **element-wise**.\n",
        "* In AI ‚Üí Combine features from different sources.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* Vector1 = `[2, 3]` (student A‚Äôs scores in Math, Physics)\n",
        "* Vector2 = `[4, 1]` (student B‚Äôs scores)\n",
        "* Vector1 + Vector2 = `[6, 4]`\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Scalar Multiplication**\n",
        "\n",
        "* Multiply vector by a number ‚Üí scales it up or down.\n",
        "* In AI ‚Üí Normalization, scaling embeddings.\n",
        "\n",
        "üëâ Example:\n",
        "`2 * [3, 4] = [6, 8]`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Dot Product (Inner Product)**\n",
        "\n",
        "* Formula:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\n",
        "  $$\n",
        "* Measures **similarity** between two vectors.\n",
        "* **Core in AI**:\n",
        "\n",
        "  * Cosine similarity for recommendation systems.\n",
        "  * Attention mechanism in transformers.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* `a = [1, 2, 3]`\n",
        "* `b = [4, 5, 6]`\n",
        "* Dot product = `1*4 + 2*5 + 3*6 = 32`\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Norm (Length of a Vector)**\n",
        "\n",
        "* Euclidean norm:\n",
        "\n",
        "  $$\n",
        "  \\|v\\| = \\sqrt{\\sum_{i=1}^n v_i^2}\n",
        "  $$\n",
        "* In AI ‚Üí Used in **normalization** (unit vectors).\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* `v = [3, 4]`\n",
        "* ‚Äñv‚Äñ = ‚àö(3¬≤ + 4¬≤) = 5\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Unit Vector**\n",
        "\n",
        "* A vector with length = 1.\n",
        "* Important for **direction only**, not magnitude.\n",
        "* Used in **cosine similarity**.\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Python Practice\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "v1 = np.array([2, 3])\n",
        "v2 = np.array([4, 1])\n",
        "\n",
        "# Addition & Subtraction\n",
        "print(\"Addition:\", v1 + v2)\n",
        "print(\"Subtraction:\", v1 - v2)\n",
        "\n",
        "# Scalar multiplication\n",
        "print(\"Scalar Multiplication:\", 3 * v1)\n",
        "\n",
        "# Dot Product\n",
        "print(\"Dot Product:\", np.dot(v1, v2))\n",
        "\n",
        "# Norm (magnitude of vector)\n",
        "print(\"Norm of v1:\", np.linalg.norm(v1))\n",
        "\n",
        "# Unit Vector\n",
        "unit_v1 = v1 / np.linalg.norm(v1)\n",
        "print(\"Unit vector of v1:\", unit_v1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Real AI Example\n",
        "\n",
        "* In **word embeddings**:\n",
        "  `\"king\" - \"man\" + \"woman\" ‚âà \"queen\"`\n",
        "  ‚Üí This is vector addition & subtraction in action.\n",
        "\n",
        "* In **transformers**:\n",
        "  Attention scores = dot product between query & key vectors.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 3: Matrix Operations**\n",
        "---\n",
        "\n",
        "### 1. **Matrix Addition & Subtraction**\n",
        "\n",
        "* Add/subtract element-wise (same shape required).\n",
        "* Example: Combining two feature matrices.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} +\n",
        "\\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix} =\n",
        "\\begin{bmatrix}6 & 8 \\\\ 10 & 12\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Scalar Multiplication**\n",
        "\n",
        "* Multiply each element of a matrix by a scalar.\n",
        "* Example: scaling image brightness.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Matrix Multiplication (Dot Product of Matrices)**\n",
        "\n",
        "* Core of neural networks:\n",
        "\n",
        "  $$\n",
        "  (m \\times n) \\cdot (n \\times p) = (m \\times p)\n",
        "  $$\n",
        "* Example: Features √ó Weights = Predictions.\n",
        "\n",
        "üëâ Example:\n",
        "If X = (students √ó features), W = (features √ó 1),\n",
        "then $y = XW$ = predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Transpose (A·µÄ)**\n",
        "\n",
        "* Flips matrix across diagonal (rows ‚Üî columns).\n",
        "* Used in covariance matrices, attention (Q¬∑K·µÄ).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Identity Matrix (I)**\n",
        "\n",
        "* Acts like **1** in multiplication.\n",
        "* Example: AI networks initialize near identity sometimes to preserve stability.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Inverse (A‚Åª¬π)**\n",
        "\n",
        "* If $A \\cdot A^{-1} = I$.\n",
        "* Not all matrices have inverses (singular ones don‚Äôt).\n",
        "* Used in solving linear equations.\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Python Practice\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[2, 0],\n",
        "              [1, 2]])\n",
        "\n",
        "# Addition & Subtraction\n",
        "print(\"A + B:\\n\", A + B)\n",
        "print(\"A - B:\\n\", A - B)\n",
        "\n",
        "# Scalar multiplication\n",
        "print(\"2 * A:\\n\", 2 * A)\n",
        "\n",
        "# Matrix Multiplication\n",
        "print(\"A * B (dot product):\\n\", np.dot(A, B))\n",
        "\n",
        "# Transpose\n",
        "print(\"Transpose of A:\\n\", A.T)\n",
        "\n",
        "# Identity Matrix\n",
        "I = np.eye(2)\n",
        "print(\"Identity Matrix:\\n\", I)\n",
        "\n",
        "# Inverse\n",
        "print(\"Inverse of A:\\n\", np.linalg.inv(A))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Real AI Examples\n",
        "\n",
        "* **Matrix multiplication** = the forward pass of a neural network (inputs √ó weights).\n",
        "* **Transpose** = used in attention mechanism (`Q √ó K·µÄ`).\n",
        "* **Inverse** = solving linear regression equation:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
        "  $$\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 4: Linear Transformations**\n",
        "---\n",
        "\n",
        "### 1. **What is a Linear Transformation?**\n",
        "\n",
        "* A **linear transformation** is simply applying a **matrix** to a vector.\n",
        "* If $A$ is a matrix and $v$ is a vector:\n",
        "\n",
        "  $$\n",
        "  T(v) = A \\cdot v\n",
        "  $$\n",
        "* Effect: Changes the vector (rotates, scales, reflects, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Examples of Transformations**\n",
        "\n",
        "* **Scaling**: Multiply by a diagonal matrix.\n",
        "\n",
        "  $$\n",
        "  \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n",
        "  \\cdot\n",
        "  \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
        "  = \\begin{bmatrix} 2x \\\\ 3y \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  ‚Üí stretches x by 2, y by 3.\n",
        "\n",
        "* **Rotation**:\n",
        "\n",
        "  $$\n",
        "  R(\\theta) =\n",
        "  \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  Rotates vector by angle Œ∏.\n",
        "\n",
        "* **Reflection**:\n",
        "  Across x-axis:\n",
        "\n",
        "  $$\n",
        "  \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Why Important for AI?**\n",
        "\n",
        "* **Images**: Scaling/rotation = matrix transformation.\n",
        "* **Embeddings**: Transform vectors into new spaces.\n",
        "* **Neural Networks**: Every layer is just a transformation $y = Wx + b$.\n",
        "* **PCA**: Projecting data to new axes = linear transformation.\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Python Practice\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# A vector\n",
        "v = np.array([2, 3])\n",
        "\n",
        "# Scaling (stretch x by 2, y by 3)\n",
        "scaling = np.array([[2, 0],\n",
        "                    [0, 3]])\n",
        "print(\"Scaled vector:\", np.dot(scaling, v))\n",
        "\n",
        "# Rotation by 90 degrees (Œ∏ = 90¬∞ = œÄ/2)\n",
        "theta = np.pi / 2\n",
        "rotation = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                     [np.sin(theta),  np.cos(theta)]])\n",
        "print(\"Rotated vector (90¬∞):\", np.dot(rotation, v))\n",
        "\n",
        "# Reflection across x-axis\n",
        "reflection = np.array([[1, 0],\n",
        "                       [0, -1]])\n",
        "print(\"Reflected vector:\", np.dot(reflection, v))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Real AI Example\n",
        "\n",
        "* **Image augmentation**: flipping, rotating, scaling images before training = matrix transformations.\n",
        "* **Word embeddings**: analogy solving (‚Äúking - man + woman ‚âà queen‚Äù) is basically linear transformations in vector space.\n",
        "* **Neural nets**: Each hidden layer is applying a transformation with a weight matrix.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 5: Determinant & Rank**\n",
        "---\n",
        "\n",
        "## 1. **Determinant (det(A))**\n",
        "\n",
        "* A single number that tells us:\n",
        "\n",
        "  * If the matrix is **invertible** (non-zero determinant).\n",
        "  * How a transformation **scales area/volume**.\n",
        "* **Geometric meaning**:\n",
        "\n",
        "  * det = 0 ‚Üí transformation **collapses space** (loses info).\n",
        "  * det = 1 ‚Üí preserves area/volume.\n",
        "  * det = -1 ‚Üí preserves area but flips orientation.\n",
        "\n",
        "üëâ Example in AI:\n",
        "\n",
        "* In **linear regression**, we compute:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
        "  $$\n",
        "\n",
        "  ‚Üí Works only if det($X^TX$) ‚â† 0 (so it has an inverse).\n",
        "* In **PCA**, covariance matrix‚Äôs determinant tells if features are redundant.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Rank (rank(A))**\n",
        "\n",
        "* Rank = **number of independent rows or columns**.\n",
        "* Maximum rank = min(rows, cols).\n",
        "* Rank tells us **how much information is in the matrix**.\n",
        "\n",
        "üëâ Example in AI:\n",
        "\n",
        "* If dataset matrix $X$ has **low rank**, some features are linear combinations of others (redundant).\n",
        "* **Rank deficiency** means the model can‚Äôt learn uniquely (important in regression & embeddings).\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Python Practice\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[2, 1],\n",
        "              [4, 2]])   # Second row is multiple of first row\n",
        "\n",
        "B = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "\n",
        "# Determinant\n",
        "print(\"det(A):\", np.linalg.det(A))  # Should be 0 (rows dependent)\n",
        "print(\"det(B):\", np.linalg.det(B))\n",
        "\n",
        "# Rank\n",
        "print(\"Rank of A:\", np.linalg.matrix_rank(A))\n",
        "print(\"Rank of B:\", np.linalg.matrix_rank(B))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Real AI Example\n",
        "\n",
        "* **Rank Deficiency**: In linear regression, if features are collinear (like height in cm and height in inches), $X^TX$ becomes singular (det=0) ‚Üí can‚Äôt invert.\n",
        "* **PCA**: The rank of the covariance matrix tells how many meaningful dimensions exist.\n",
        "* **Neural Nets**: Low-rank approximations are used to **compress large models** (reduce parameters).\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 6: Eigenvalues & Eigenvectors**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Definition**\n",
        "\n",
        "For a square matrix $A$, an **eigenvector** $v$ is a vector that only gets **stretched (scaled)** when multiplied by $A$, not rotated.\n",
        "\n",
        "$$\n",
        "A v = \\lambda v\n",
        "$$\n",
        "\n",
        "* $v$ = eigenvector\n",
        "* $\\lambda$ = eigenvalue (the stretch factor)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Intuition**\n",
        "\n",
        "* Imagine a transformation (rotation, scaling). Most vectors will change **direction + length**.\n",
        "* Eigenvectors are **special directions** that **don‚Äôt change direction** (just scaled).\n",
        "* Eigenvalues tell **how much scaling happens**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Why Important in AI**\n",
        "\n",
        "* **PCA (Principal Component Analysis)**: Eigenvectors of covariance matrix = principal directions of data, eigenvalues = variance explained.\n",
        "* **Google PageRank**: Eigenvector of link matrix.\n",
        "* **Stability analysis**: Eigenvalues help check if a system converges.\n",
        "* **Spectral clustering**: Uses eigenvectors of graph Laplacians.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Python Example**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Matrix\n",
        "A = np.array([[4, 2],\n",
        "              [1, 3]])\n",
        "\n",
        "# Eigen decomposition\n",
        "eig_vals, eig_vecs = np.linalg.eig(A)\n",
        "\n",
        "print(\"Eigenvalues:\", eig_vals)\n",
        "print(\"Eigenvectors:\\n\", eig_vecs)\n",
        "\n",
        "# Verification: A * v = Œª * v\n",
        "v = eig_vecs[:, 0]   # first eigenvector\n",
        "Œª = eig_vals[0]\n",
        "print(\"Av:\", np.dot(A, v))\n",
        "print(\"Œªv:\", Œª * v)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Geometric View**\n",
        "\n",
        "* If A = scaling matrix, eigenvectors = axes of scaling.\n",
        "* If A = rotation by 90¬∞, no real eigenvectors (everything rotates).\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Real AI Examples\n",
        "\n",
        "* **PCA**:\n",
        "\n",
        "  * Input ‚Üí covariance matrix ‚Üí eigenvectors/eigenvalues ‚Üí choose top-k eigenvectors (directions with highest variance).\n",
        "* **Face Recognition (Eigenfaces)**: Uses PCA to reduce dimensionality.\n",
        "* **Neural Nets**: Hessian matrix‚Äôs eigenvalues tell about convergence speed (flat vs sharp minima).\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **Topic 7: Singular Value Decomposition (SVD)**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **What is SVD?**\n",
        "\n",
        "For any $m \\times n$ matrix $A$, SVD decomposes it into three matrices:\n",
        "\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "* **U** ‚Üí $m \\times m$ orthogonal matrix (left singular vectors)\n",
        "* **Œ£** ‚Üí $m \\times n$ diagonal matrix (singular values)\n",
        "* **V·µÄ** ‚Üí $n \\times n$ orthogonal matrix (right singular vectors)\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "* Think of it as **rotating ‚Üí scaling ‚Üí rotating back**.\n",
        "* Singular values in Œ£ tell **how much variance or ‚Äúenergy‚Äù each dimension carries**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Why Important in AI**\n",
        "\n",
        "* **PCA**: Eigenvectors of covariance matrix = right singular vectors (V), singular values = variance explained.\n",
        "* **Recommender Systems**: Decompose user-item rating matrix ‚Üí predict missing ratings.\n",
        "* **Image Compression**: Keep top-k singular values ‚Üí approximate image with fewer numbers.\n",
        "* **Dimensionality Reduction**: Reduce large datasets with minimal information loss.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Python Example**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Matrix\n",
        "A = np.array([[3, 1, 1],\n",
        "              [-1, 3, 1]])\n",
        "\n",
        "# SVD decomposition\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "\n",
        "print(\"U:\\n\", U)\n",
        "print(\"Singular values:\", S)\n",
        "print(\"Vt:\\n\", Vt)\n",
        "\n",
        "# Reconstruct original matrix (using all singular values)\n",
        "Sigma = np.zeros((A.shape[0], A.shape[1]))\n",
        "Sigma[:len(S), :len(S)] = np.diag(S)\n",
        "A_reconstructed = np.dot(U, np.dot(Sigma, Vt))\n",
        "print(\"Reconstructed A:\\n\", A_reconstructed)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Geometric Intuition**\n",
        "\n",
        "* **U** ‚Üí new basis for **rows**\n",
        "* **V** ‚Üí new basis for **columns**\n",
        "* **Œ£** ‚Üí scales each dimension\n",
        "* Imagine a **rubber sheet**: rotate it ‚Üí stretch ‚Üí rotate back ‚Üí original shape recovered.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Real AI Examples**\n",
        "\n",
        "* **PCA for dimensionality reduction**: Keep top-k singular values ‚Üí reduce features while retaining maximum variance.\n",
        "* **Movie Recommendation**:\n",
        "\n",
        "  * User-Item matrix ‚Üí SVD ‚Üí predict unknown ratings.\n",
        "* **Image Compression**:\n",
        "\n",
        "  * Keep largest singular values ‚Üí smaller storage, minimal loss.\n",
        "* **Neural Networks**: Low-rank approximation of weight matrices ‚Üí smaller models.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "O3ldUsCDA5p9"
      }
    }
  ]
}